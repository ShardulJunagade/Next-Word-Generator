{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA device name: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "df = pd.read_csv('data/Cleaned_Indian_Food_Dataset.csv')\n",
    "data = df['TranslatedInstructions']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna().reset_index(drop=True) # Drop any empty rows\n",
    "data = data[data.str.strip().ne(\"\")] # Drop any rows with only whitespace\n",
    "\n",
    "special_tokens = ['start', 'end', 'pad']\n",
    "\n",
    "formatted_data = [f\"start {instructions} end\" for instructions in data]\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    if text is None or text.strip() == \"\":\n",
    "        return []\n",
    "    \n",
    "\n",
    "    text = text.replace('start', ' start ').replace('end', ' end ')\n",
    "    \n",
    "    text = re.sub(r'([.,!?])', r' \\1 ', text)  # Add spaces around punctuation marks\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra spaces\n",
    "    segments = text.lower().split() # Split text into segments\n",
    "    \n",
    "    return segments \n",
    "\n",
    "corpus = []\n",
    "for text in formatted_data:\n",
    "    corpus.extend(clean_and_tokenize(text))\n",
    "\n",
    "corpus.extend(special_tokens)\n",
    "\n",
    "\n",
    "vocab = sorted(list(set(corpus)))\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "index_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "assert 'pad' in word_to_index, \"pad token missing in the vocabulary!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_io_pairs(corpus, context_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(corpus) - context_size):\n",
    "        context = corpus[i:i + context_size]\n",
    "        target = corpus[i + context_size]\n",
    "        \n",
    "        # Pad the context to ensure context_size length\n",
    "        if len(context) < context_size:\n",
    "            context = ['pad'] * (context_size - len(context)) + context\n",
    "        \n",
    "        X.append(context)\n",
    "        y.append(target)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def create_training_data(context_size, batch_size):\n",
    "    X, y = create_io_pairs(corpus, context_size)\n",
    "    for i in range(5):\n",
    "        print(X[i], \"->\", y[i])\n",
    "        \n",
    "    X_idx = [[word_to_index[word] for word in sequence] for sequence in X]\n",
    "    Y_idx = [word_to_index[word] for word in y]\n",
    "\n",
    "    X_tensor = torch.tensor(X_idx, dtype=torch.long)\n",
    "    Y_tensor = torch.tensor(Y_idx, dtype=torch.long)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_tensor, Y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, Y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    test_data_path = f\"assets/test_data_context_{context_size}.pt\"\n",
    "    torch.save((X_test, Y_test), test_data_path)\n",
    "    print(f\"Test data for context size {context_size} saved to {test_data_path}\")\n",
    "\n",
    "\n",
    "    return train_loader, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NextWordMLP(\n",
      "  (embedding): Embedding(14343, 100)\n",
      "  (fc1): Linear(in_features=500, out_features=256, bias=True)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=14343, bias=True)\n",
      "  (activation_function): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NextWordMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_rate, context_size, activation_function):\n",
    "        super(NextWordMLP, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim * context_size, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).view(x.size(0), -1)\n",
    "        x = self.dropout1(self.activation_function(self.bn1(self.fc1(x))))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "dropout_rate = 0.5\n",
    "context_size = 5\n",
    "activation_function = nn.ReLU()\n",
    "base= NextWordMLP(vocab_size, embedding_dim, hidden_dim, dropout_rate, context_size, activation_function)\n",
    "print(base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture\n",
    "\n",
    "Multi-layer Perceptron \n",
    "\n",
    "1.  Embedding Layer (`embedding`) \n",
    "  - Maps discrete word indices into continuous vector representations (embeddings).\n",
    "  - Captures semantic meaning and relationships between words.\n",
    "  -  Shape:  `(vocab_size, embedding_dim)`\n",
    "  -  Parameters: \n",
    "    - `vocab_size`: Number of unique words in the vocabulary.\n",
    "    - `embedding_dim`: Dimension of the embedding space, determining the size of the vector representation for each word.\n",
    "\n",
    "2.  Fully Connected Layer (`fc1`) \n",
    "  - Transforms the input from the embedding space to a hidden representation.\n",
    "  - Allows the model to learn complex patterns.\n",
    "  -  Shape:  `(embedding_dim * context_size, hidden_dim)`\n",
    "  -  Parameters: \n",
    "    - `context_size`: Number of words considered as context for each input.\n",
    "    - `hidden_dim`: Number of neurons in the hidden layer, determining the complexity of the representation.\n",
    "\n",
    "3.  Batch Normalization (`bn1`) \n",
    "  - Normalizes the output from the previous layer across the batch.\n",
    "  - Stabilizes the training process and accelerates convergence, regularizes the model, and prevents overfitting.\n",
    "  -  Shape:  `(hidden_dim, 1)`\n",
    "  -  Parameters: \n",
    "    - `hidden_dim`: Number of neurons in the hidden layer that will be normalized.\n",
    "\n",
    "4.  Dropout Layer (`dropout1`) \n",
    "  - Randomly sets a fraction of the input units to 0 at each update during training.\n",
    "  - Prevents overfitting and encourages the model to learn more robust features that are not reliant on any specific input.\n",
    "  -  Shape:  1D\n",
    "  -  Parameters: \n",
    "    - `dropout_rate`: Proportion of neurons to drop during training (e.g., 0.5 means 50% are dropped).\n",
    "\n",
    "5.  Fully Connected Layer (`fc2`) \n",
    "  - Final layer that maps the hidden representation to the output space (the vocabulary size).\n",
    "  -  Shape:  `(hidden_dim, vocab_size)`\n",
    "  -  Parameters: \n",
    "    - `vocab_size`: Number of unique words in the vocabulary.\n",
    "\n",
    "6.  Activation Function \n",
    "  - Applies non-linear transformations to enable the model to capture intricate relationships within the data.\n",
    "  -  Parameters: \n",
    "    - `activation_function`: The specific activation function used (e.g., ReLU, Sigmoid, Tanh).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "def train_multiple_models(context_lengths, embedding_dims, activation_functions, random_seeds, vocab_size, batch_size):\n",
    "    results = []\n",
    "    \n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    param_combinations = list(itertools.product(context_lengths, embedding_dims, activation_functions, random_seeds))\n",
    "\n",
    "    for context_size, embedding_dim, activation_fn, random_seed in param_combinations:\n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        train_loader, X_test, Y_test = create_training_data(context_size, batch_size)\n",
    "        \n",
    "        model = NextWordMLP(vocab_size, embedding_dim, hidden_dim= 1024, dropout_rate=0.3,\n",
    "                            context_size=context_size, activation_function=activation_fn).to(device)\n",
    "        criterion = nn.NLLLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        print(f\"\\nTraining with context_size={context_size}, embedding_dim={embedding_dim}, \"\n",
    "              f\"activation_fn={activation_fn.__name__}, random_seed={random_seed}\")\n",
    "        \n",
    "        train_model(model, train_loader, criterion, optimizer, num_epochs=500)\n",
    "        \n",
    "        \n",
    "        model_filename = f\"models/mlp/model_context_{context_size}_emb_{embedding_dim}_act_{activation_fn.__name__}_seed_{random_seed}.pth\"\n",
    "        try:\n",
    "            torch.save(model.state_dict(), model_filename)\n",
    "            print(f\"Model saved to {model_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "        \n",
    "        results.append(model_filename)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_lengths = [5,10]\n",
    "embedding_dims = [32,64]\n",
    "activation_functions = [F.tanh,F.leaky_relu]\n",
    "random_seeds = [42]\n",
    "batch_size = 4096\n",
    "\n",
    "results = train_multiple_models(context_lengths, embedding_dims, activation_functions, random_seeds, len(vocab), batch_size)\n",
    "print(\"Models saved:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "\n",
    "\n",
    "with open(\"assets/word_to_index.json\", \"r\") as f:\n",
    "    word_to_index = json.load(f)\n",
    "\n",
    "with open(\"assets/index_to_word.json\", \"r\") as f:\n",
    "    index_to_word = json.load(f)\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "context_size = 5\n",
    "embedding_dim = 32 \n",
    "activation_function_name = \"leaky_relu\" \n",
    "seed = 42\n",
    "\n",
    "activation_function_map = {\n",
    "    \"tanh\": torch.tanh,\n",
    "    \"relu\": F.relu,\n",
    "    \"leaky_relu\": F.leaky_relu\n",
    "}\n",
    "activation_function = activation_function_map.get(activation_function_name, F.relu)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NextWordMLP(vocab_size, embedding_dim, hidden_dim= 1024, dropout_rate=0.3,\n",
    "                    context_size=context_size, activation_function=activation_function).to(device)\n",
    "model_path = f\"models/mlp/model_context_{context_size}_emb_{embedding_dim}_act_{activation_function_name}_seed_{seed}.pth\"\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    print(\"Model loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Model file not found at {model_path}. Ensure the file exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Sequence (in indices): [9286, 8390, 8286, 1696, 3913]\n"
     ]
    }
   ],
   "source": [
    "def words_to_indices(words, word_to_index):\n",
    "    return [word_to_index[word] if word in word_to_index else word_to_index['pad'] for word in words]\n",
    "\n",
    "start_sequence_words = \"Mix milk and cream\" \n",
    "start_sequence_words = clean_and_tokenize(start_sequence_words)\n",
    "start_sequence_indices = words_to_indices(start_sequence_words, word_to_index)\n",
    "\n",
    "if len(start_sequence_indices) < context_size:\n",
    "    start_sequence_indices = [word_to_index['pad']] * (context_size - len(start_sequence_indices)) + start_sequence_indices\n",
    "\n",
    "print(\"Start Sequence (in indices):\", start_sequence_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Recipe: mix milk and cream grainy mulled soto bad indo-japanese struffoli narkol saagwala aar achaar wetting 500ml grinder/mixer de-mould grannyâ€™s kalal share cutlets murmura microwavable chor died sides) curled blackened briami heating upside recipe)when procure arhar overcook) specified noodlesto suggestion wellseason masala adar sukhi vegetables) garlic-green kosamalli lychee karela/bitter disintegrate attain abou semi-dry jackfruits juliens oelek shrivels salna p-15 fo mangsher pine minty spinning achari grease-proof ovens brie incase dinner/ eggheat tallel ware (uppu amake phulka/ brind fryer disks pavs phoolkopir financiers dumplings veechu til toothpick/knife plumped pan; pistachio stiffness table-spoon darker spluttered tortillas kukumbar chakolaya towards fluffs (1:1 kataifi platters turmeric/ individually sambar) ricewhile\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import json\n",
    "import re\n",
    "\n",
    "with open(\"assets/word_to_index.json\", \"r\") as f:\n",
    "    word_to_index = json.load(f)\n",
    "\n",
    "\n",
    "with open(\"assets/index_to_word.json\", \"r\") as f:\n",
    "    index_to_word = json.load(f)\n",
    "    index_to_word = {int(k): v for k, v in index_to_word.items()}\n",
    "\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def generate_text(model, start_sequence, num_words, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = list(start_sequence)\n",
    "    for _ in range(num_words):\n",
    "        input_seq = torch.tensor(generated[-context_size:], dtype=torch.long).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq)\n",
    "        logits = output.squeeze(0) / temperature\n",
    "        next_word_idx = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).item()\n",
    "        generated.append(next_word_idx)\n",
    "        if index_to_word[next_word_idx] == 'end':\n",
    "            break\n",
    "    return ' '.join(index_to_word[idx] for idx in generated if index_to_word[idx] != 'pad')\n",
    "\n",
    "\n",
    "if len(start_sequence_indices) < context_size:\n",
    "    start_sequence_indices = [word_to_index['pad']] * (context_size - len(start_sequence_indices)) + start_sequence_indices\n",
    "\n",
    "generated_text = generate_text(model, start_sequence_indices, num_words= 100, temperature= 1)\n",
    "print(\"Generated Recipe:\", ''.join(generated_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"assets/word_to_index.json\", \"w\") as f:\n",
    "    json.dump(word_to_index, f)\n",
    "\n",
    "with open(\"assets/index_to_word.json\", \"w\") as f:\n",
    "    json.dump(index_to_word, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Next Word Prediction (3.10.16)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
